= Creating a ROSA cluster with Private Link enabled (custom VPC) and STS

This is a combination of the private-link and sts setup documents to show the full picture

.Architecture diagram showing privatelink with public subnet
image::../media/private-link.png[width=100%]

== AWS Preparation

. Run the following to check for the AWS Load Balancer role and create it if it is missing.

[source,sh,role=copy]
----
aws iam get-role --role-name "AWSServiceRoleForElasticLoadBalancing" || aws iam create-service-linked-role --aws-service-name "elasticloadbalancing.amazonaws.com"
----

== Create the AWS Virtual Private Cloud (VPC) and Subnets

For this scenario, we will be using a newly created VPC with both public and private subnets. All of the cluster resources will reside in the private subnet. The public subnet will be used for traffic to the Internet (egress).

// Use `rosa list instance-types` to list the available ROSA instance types.

// Use `aws ec2 describe-instance-type-offerings` to check that your desired AZ supports your desired instance type.

// Example using us-west-1, us-west-1a, and m5a.xlarge:  

// [source,sh,role=copy]
// ----
// aws ec2 describe-instance-type-offerings --location-type availability-zone \
//   --region us-west-1 \
//   --filters Name=location,Values=us-west-1a \
//   --output text | egrep m5a.xlarge
// ----

// .Sample Output
// [source,texinfo]
// ----
// INSTANCETYPEOFFERINGS	m5a.xlarge	us-west-1a	availability-zone
// ----

// The result should display `INSTANCETYPEOFFERINGS [instance-type] [az] availability-zone` if your selected region supports your desired instance type (like in the example above.)

. Configure the following environment variables:
+
[source,sh,role=copy]
----
PL_CLUSTER_NAME=pl-sts-${GUID}
PL_REGION=us-west-1 # Make sure to use a region where you haven't deployed a cluster yet
PL_VERSION=4.13.4

AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
AWS_PAGER=""

touch ${HOME}/pl-vars
echo "export PL_CLUSTER_NAME=${PL_CLUSTER_NAME}" >>${HOME}/pl-vars
echo "export PL_REGION=${PL_REGION}" >>${HOME}/pl-vars
echo "export PL_VERSION=${PL_VERSION}" >>${HOME}/pl-vars
echo "export AWS_ACCOUNT_ID=${AWS_ACCOUNT_ID}" >>${HOME}/pl-vars
----

. Create a VPC for use by ROSA
.. Create the VPC and return the ID as PL_VPC_ID
+
[source,sh,role=copy]
----
PL_VPC_ID=$(aws ec2 create-vpc --cidr-block 10.0.0.0/16 --region ${PL_REGION} | jq -r .Vpc.VpcId)

echo "export PL_VPC_ID=${PL_VPC_ID}" >>${HOME}/pl-vars
echo ${PL_VPC_ID}
----
+
.Sample Output
[source,texinfo]
----
vpc-0089636f591bd8b57
----

.. Tag the newly created VPC with the cluster name
+
[source,sh,role=copy]
----
aws ec2 create-tags --resources ${PL_VPC_ID} \
  --region ${PL_REGION} \
  --tags Key=Name,Value=${PL_CLUSTER_NAME}
----

.. Configure the VPC to allow DNS hostnames for their public IP addresses
+
[source,sh,role=copy]
----
aws ec2 modify-vpc-attribute \
  --region ${PL_REGION} \
  --vpc-id ${PL_VPC_ID} \
  --enable-dns-hostnames
----

. Create a Public Subnet to allow egress traffic to the Internet
.. Create the public subnet in the VPC CIDR block range and return the ID as PUBLIC_SUBNET
+
[source,sh,role=copy]
----
PL_PUBLIC_SUBNET=$(aws ec2 create-subnet \
  --region ${PL_REGION} \
  --vpc-id ${PL_VPC_ID} \
  --cidr-block 10.0.128.0/24 | jq -r .Subnet.SubnetId)

echo "export PL_PUBLIC_SUBNET=${PL_PUBLIC_SUBNET}" >>${HOME}/pl-vars

echo ${PL_PUBLIC_SUBNET}
----
+
.Sample Output
[source,texinfo]
----
subnet-07665542bc85bc092
----

.. Tag the public subnet with the cluster name
+
[source,sh,role=copy]
----
aws ec2 create-tags \
  --region ${PL_REGION} \
  --resources ${PL_PUBLIC_SUBNET} \
  --tags Key=Name,Value=${PL_CLUSTER_NAME}-public
----

. Create a Private Subnet for the cluster
.. Create the private subnet in the VPC CIDR block range and return the ID as PL_PRIVATE_SUBNET
+
[source,sh,role=copy]
----
PL_PRIVATE_SUBNET=$(aws ec2 create-subnet \
  --region ${PL_REGION} \
  --vpc-id ${PL_VPC_ID} \
  --cidr-block 10.0.0.0/17 | jq -r .Subnet.SubnetId)

echo "export PL_PRIVATE_SUBNET=${PL_PRIVATE_SUBNET}" >>${HOME}/pl-vars

echo ${PL_PRIVATE_SUBNET}
----
+
.Sample Output
[source,texinfo]
----
subnet-0299591443c7eb1d8
----

.. Tag the private subnet with the cluster name
+
[source,sh,role=copy]
----
aws ec2 create-tags \
  --region ${PL_REGION} \
  --resources ${PL_PRIVATE_SUBNET} \
  --tags Key=Name,Value=${PL_CLUSTER_NAME}-private
----

.. Both subnets should now be visible in the AWS console

. Create an Internet Gateway for NAT egress traffic

.. Create the Internet Gateway and return the ID as PL_IGW
+
[source,sh,role=copy]
----
PL_IGW=$(aws ec2 create-internet-gateway --region ${PL_REGION} | jq -r .InternetGateway.InternetGatewayId)

echo "export PL_IGW=${PL_IGW}" >>${HOME}/pl-vars

echo ${PL_IGW}
----
+
.Sample Output
[source,texinfo]
----
igw-0309b87ea4bddc5f4
----

.. Attach the new Internet Gateway to the VPC
+
[source,sh,role=copy]
----
aws ec2 attach-internet-gateway \
  --region ${PL_REGION} \
  --vpc-id ${PL_VPC_ID} \
  --internet-gateway-id ${PL_IGW}
----

.. Tag the Internet Gateway with the cluster name
+
[source,sh,role=copy]
----
aws ec2 create-tags \
  --region ${PL_REGION} \
  --resources ${PL_IGW} \
  --tags Key=Name,Value=${PL_CLUSTER_NAME}
----

.. The new Internet Gateway should be created and attached to your VPC

. Create a Route Table for NAT egress traffic
.. Create the Route Table and return the ID as PL_RTB
+
[source,sh,role=copy]
----
PL_RTB=$(aws ec2 create-route-table --region ${PL_REGION} --vpc-id ${PL_VPC_ID} | jq -r .RouteTable.RouteTableId)

echo "export PL_RTB=${PL_RTB}" >>${HOME}/pl-vars

echo ${PL_RTB}
----
+
.Sample Output
[source,texinfo]
----
rtb-04c9cd43781c67135
----

.. Create a route with no IP limitations (0.0.0.0/0) to the Internet Gateway
+
[source,sh,role=copy]
----
aws ec2 create-route \
  --region ${PL_REGION} \
  --route-table-id ${PL_RTB} \
  --destination-cidr-block 0.0.0.0/0 \
  --gateway-id ${PL_IGW}
----
+
.Sample Output
[source,texinfo]
----
{
    "Return": true
}
----

.. Verify the route table settings
+
[source,sh,role=copy]
----
aws ec2 describe-route-tables \
  --region ${PL_REGION} \
  --route-table-id ${PL_RTB}
----
+
.Sample Output
[source,texinfo]
----
{
    "RouteTables": [
        {
            "Associations": [],
            "PropagatingVgws": [],
            "RouteTableId": "rtb-04c9cd43781c67135",
            "Routes": [
                {
                    "DestinationCidrBlock": "10.0.0.0/16",
                    "GatewayId": "local",
                    "Origin": "CreateRouteTable",
                    "State": "active"
                },
                {
                    "DestinationCidrBlock": "0.0.0.0/0",
                    "GatewayId": "igw-0309b87ea4bddc5f4",
                    "Origin": "CreateRoute",
                    "State": "active"
                }
            ],
            "Tags": [],
            "VpcId": "vpc-0089636f591bd8b57",
            "OwnerId": "870651848115"
        }
    ]
}
----

.. Associate the Route Table with the Public subnet
+
[source,sh,role=copy]
----
aws ec2 associate-route-table \
  --region ${PL_REGION} \
  --subnet-id ${PL_PUBLIC_SUBNET} \
  --route-table-id ${PL_RTB}
----
+
.Sample Output
[source,texinfo]
----
{
    "AssociationId": "rtbassoc-026c82779f7365623",
    "AssociationState": {
        "State": "associated"
    }
}
----

.. Tag the Route Table with the cluster name
+
[source,sh,role=copy]
----
aws ec2 create-tags \
  --region ${PL_REGION} \
  --resources ${PL_RTB} \
  --tags Key=Name,Value=${PL_CLUSTER_NAME}
----

. Create a NAT Gateway for the Private network
.. Allocate and elastic IP address and return the ID as PL_EIP
+
[source,sh,role=copy]
----
PL_EIP=$(aws ec2 allocate-address --region ${PL_REGION} --domain vpc | jq -r .AllocationId)

echo "export PL_EIP=${PL_EIP}" >>${HOME}/pl-vars

echo ${PL_EIP}
----
+
.Sample Output
[source,texinfo]
----
eipalloc-0543777bb726d925a
----

.. Create a new NAT Gateway in the Public subnet with the new Elastic IP address and return the ID as PL_NAT
+
[source,sh,role=copy]
----
PL_NAT=$(aws ec2 create-nat-gateway \
  --region ${PL_REGION} \
  --subnet-id ${PL_PUBLIC_SUBNET} \
  --allocation-id ${PL_EIP} | jq -r .NatGateway.NatGatewayId)

echo "export PL_NAT=${PL_NAT}" >>${HOME}/pl-vars

echo ${PL_NAT}
----
+
.Sample Output
[source,texinfo]
----
nat-0a0f9ee1d8c5941e2
----

.. Tag the Elastic IP and NAT gateway with the cluster name
+
[source,sh,role=copy]
----
aws ec2 create-tags \
  --region ${PL_REGION} \
  --resources ${PL_EIP} \
  --resources ${PL_NAT} \
  --tags Key=Name,Value=${PL_CLUSTER_NAME}
----

.. The new NAT Gateway should be created and associated with your VPC

. Create a Route Table for the Private subnet to the NAT Gateway
.. Create a Route Table in the VPC and return the ID as PL_RTB_NAT
+
[source,sh,role=copy]
----
PL_RTB_NAT=$(aws ec2 create-route-table --region ${PL_REGION} --vpc-id ${PL_VPC_ID} \
  | jq -r .RouteTable.RouteTableId)

echo "export PL_RTB_NAT=${PL_RTB_NAT}" >>${HOME}/pl-vars

echo ${PL_RTB_NAT}
----
+
.Sample Output
[source,texinfo]
----
rtb-010ff0fd5626fddfb
----

.. Loop through a Route Table check until it is created
+
[source,sh,role=copy]
----
while ! aws ec2 describe-route-tables --region ${PL_REGION} --route-table-id ${PL_RTB_NAT} | jq .; do sleep 1; done
----
+
.Sample Output
[source,texinfo]
----
{
  "RouteTables": [
    {
      "Associations": [],
      "PropagatingVgws": [],
      "RouteTableId": "rtb-010ff0fd5626fddfb",
      "Routes": [
        {
          "DestinationCidrBlock": "10.0.0.0/16",
          "GatewayId": "local",
          "Origin": "CreateRouteTable",
          "State": "active"
        }
      ],
      "Tags": [],
      "VpcId": "vpc-0089636f591bd8b57",
      "OwnerId": "870651848115"
    }
  ]
}
----

.. Create a route in the new Route Table for all addresses to the NAT Gateway
+
[source,sh,role=copy]
----
aws ec2 create-route \
  --region ${PL_REGION} \
  --route-table-id ${PL_RTB_NAT} \
  --destination-cidr-block 0.0.0.0/0 \
  --gateway-id ${PL_NAT}
----
+
.Sample Output
[source,texinfo]
----
{
    "Return": true
}
----

.. Associate the Route Table with the Private subnet
+
[source,sh,role=copy]
----
aws ec2 associate-route-table \
  --region ${PL_REGION} \
  --subnet-id ${PL_PRIVATE_SUBNET} \
  --route-table-id ${PL_RTB_NAT}
----
+
.Sample Output
[source,texinfo]
----
{
    "AssociationId": "rtbassoc-0a442b015541e69d4",
    "AssociationState": {
        "State": "associated"
    }
}
----

.. Tag the Route Table with the cluster name
+
[source,sh,role=copy]
----
aws ec2 create-tags \
  --region ${PL_REGION} \
  --resources ${PL_RTB_NAT} \
  --tags Key=Name,Value=${PL_CLUSTER_NAME}-private
----

== Configure the AWS Security Token Service (STS) for use with ROSA

The AWS Security Token Service (STS) allows us to deploy ROSA without needing a ROSA admin account, instead it uses roles and policies to gain access to the AWS resources needed to install and operate the cluster.

This is a summary of the official OpenShift docs that can be used as a line by line install guide.

// Note that some commands (OIDC for STS) will be hard coded to US-EAST-1, do not be tempted to change these to use $PL_REGION instead or you will fail installation.

. Create the IAM Account Roles
+
[source,sh,role=copy]
----
rosa create account-roles --region ${PL_REGION} --mode auto --yes
----
+
.Sample Output
[source,texinfo]
----
I: Logged in as 'rhpds-cloud' on 'https://api.openshift.com'
I: Validating AWS credentials...
I: AWS credentials are valid!
I: Validating AWS quota...
I: AWS quota ok. If cluster installation fails, validate actual AWS resource usage against https://docs.openshift.com/rosa/rosa_getting_started/rosa-required-aws-service-quotas.html
I: Verifying whether OpenShift command-line tool is available...
I: Current OpenShift Client Version: 4.13.6
I: Creating account roles
I: Creating classic account roles using 'arn:aws:iam::858858614682:user/wkulhane@redhat.com-d4lf4'
I: Created role 'ManagedOpenShift-Installer-Role' with ARN 'arn:aws:iam::858858614682:role/ManagedOpenShift-Installer-Role'
I: Created role 'ManagedOpenShift-ControlPlane-Role' with ARN 'arn:aws:iam::858858614682:role/ManagedOpenShift-ControlPlane-Role'
I: Created role 'ManagedOpenShift-Worker-Role' with ARN 'arn:aws:iam::858858614682:role/ManagedOpenShift-Worker-Role'
I: Created role 'ManagedOpenShift-Support-Role' with ARN 'arn:aws:iam::858858614682:role/ManagedOpenShift-Support-Role'
I: To create an OIDC Config, run the following command:
	rosa create oidc-config
----

== Deploy ROSA cluster

. Run the rosa cli to create your cluster.
// WK: add --auto to also create oidc and operator roles
+
[source,sh,role=copy]
----
rosa create cluster \
  --cluster-name ${PL_CLUSTER_NAME} \
  --region ${PL_REGION} \
  --version ${PL_VERSION} \
  --subnet-ids=${PL_PRIVATE_SUBNET} \
  --private-link --machine-cidr=10.0.0.0/16 \
  --yes \
  --sts
----
+
.Sample Output
[source,texinfo]
----
W: In a future release STS will be the default mode.
W: --sts flag won't be necessary if you wish to use STS.
W: --non-sts/--mint-mode flag will be necessary if you do not wish to use STS.
I: Using arn:aws:iam::870651848115:role/ManagedOpenShift-Installer-Role for the Installer role
I: Using arn:aws:iam::870651848115:role/ManagedOpenShift-ControlPlane-Role for the ControlPlane role
I: Using arn:aws:iam::870651848115:role/ManagedOpenShift-Worker-Role for the Worker role
I: Using arn:aws:iam::870651848115:role/ManagedOpenShift-Support-Role for the Support role
W: No OIDC Configuration found; will continue with the classic flow.
W: You are choosing to use AWS PrivateLink for your cluster. STS clusters can only be private if AWS PrivateLink is used. Once the cluster is created, this option cannot be changed.
I: Creating cluster 'pl-sts-szrxx'

[...Output Omitted...]

I:
Run the following commands to continue the cluster creation:

	rosa create operator-roles --cluster pl-sts-szrxx
	rosa create oidc-provider --cluster pl-sts-szrxx

I: To determine when your cluster is Ready, run 'rosa describe cluster -c pl-sts-szrxx'.
I: To watch your cluster installation logs, run 'rosa logs install -c pl-sts-szrxx --watch'.
----

. Create the Operator Roles
+
[source,sh,role=copy]
----
rosa create operator-roles \
  --region ${PL_REGION} \
  --cluster ${PL_CLUSTER_NAME} \
  --mode auto \
  --yes
----
+
.Sample Output
[source,texinfo]
----
I: Creating roles using 'arn:aws:iam::870651848115:user/wkulhane@redhat.com-szrxx'
I: Created role 'pl-sts-szrxx-f3e4-openshift-cloud-network-config-controller-clou' with ARN 'arn:aws:iam::870651848115:role/pl-sts-szrxx-f3e4-openshift-cloud-network-config-controller-clou'
I: Created role 'pl-sts-szrxx-f3e4-openshift-machine-api-aws-cloud-credentials' with ARN 'arn:aws:iam::870651848115:role/pl-sts-szrxx-f3e4-openshift-machine-api-aws-cloud-credentials'
I: Created role 'pl-sts-szrxx-f3e4-openshift-cloud-credential-operator-cloud-cred' with ARN 'arn:aws:iam::870651848115:role/pl-sts-szrxx-f3e4-openshift-cloud-credential-operator-cloud-cred'
I: Created role 'pl-sts-szrxx-f3e4-openshift-image-registry-installer-cloud-crede' with ARN 'arn:aws:iam::870651848115:role/pl-sts-szrxx-f3e4-openshift-image-registry-installer-cloud-crede'
I: Created role 'pl-sts-szrxx-f3e4-openshift-ingress-operator-cloud-credentials' with ARN 'arn:aws:iam::870651848115:role/pl-sts-szrxx-f3e4-openshift-ingress-operator-cloud-credentials'
I: Created role 'pl-sts-szrxx-f3e4-openshift-cluster-csi-drivers-ebs-cloud-creden' with ARN 'arn:aws:iam::870651848115:role/pl-sts-szrxx-f3e4-openshift-cluster-csi-drivers-ebs-cloud-creden'
----

. Create the OIDC provider
+
[source,sh,role=copy]
----
rosa create oidc-provider \
  --cluster ${PL_CLUSTER_NAME} \
  --mode auto \
  --yes
----
+
.Sample Output
[source,texinfo]
----
I: Creating OIDC provider using 'arn:aws:iam::870651848115:user/wkulhane@redhat.com-szrxx'
I: Created OIDC provider with ARN 'arn:aws:iam::870651848115:oidc-provider/rh-oidc.s3.us-east-1.amazonaws.com/2580qblb5efub2s84ogh2cfhhiqriumq'
----

. Validate that the cluster is now installing. The *State* should have moved beyond pending and show `validating`, `installing` or `ready`.
+
[source,sh,role=copy]
----
watch -n 10 "rosa describe cluster --cluster ${PL_CLUSTER_NAME} | grep State"
----
+
.Sample Output
[source,texinfo]
----
State:                      installing (Cluster is installing)
----

. Once the *State* shows as `installing` you can watch the install logs
+
[source,sh,role=copy]
----
rosa logs install --cluster ${PL_CLUSTER_NAME} --watch
----

== Create Admin User

. Run this command to create the admin user
+
[source,sh,role=copy]
----
rosa create admin --cluster ${PL_CLUSTER_NAME} | tee $HOME/${PL_CLUSTER_NAME}.cmd
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
INFO: Admin account has been added to cluster 'pl-sts-tfbjn'.
INFO: Please securely store this generated password. If you lose this password you can delete and recreate the cluster admin user.
INFO: To login, run the following command:

   oc login https://api.pl-sts-tfbjn.ulvs.p1.openshiftapps.com:6443 --username cluster-admin --password Cbjyz-DWKc8-ov9nf-pJnVV

INFO: It may take several minutes for this access to become active.----
+
[TIP]
====
The command above also saves the login command to the `$HOME/${CLUSTER_NAME}.cmd` file for future reference.

[WARNING]
====
You can not use that user to log into your cluster yet - you do not have a way to connect to your cluster...
====

== Next steps

In order to use your cluster you must create a way to access your cluster. In this environment you currently have two options:

* Jumpbox
* VPN

Pick one of the two labs to run through next to access your cluster.
